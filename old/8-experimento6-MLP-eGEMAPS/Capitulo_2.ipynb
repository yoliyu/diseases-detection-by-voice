{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrón simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementa el algoritmo del perceptrón simple y utilízalo en los siguientes casos. $X_{1}$ y $X_{2}$ son las entradas al perceptrón y $d$ la señal de salida.\n",
    "\n",
    "**Puerta AND:**\n",
    "| $X_{1}$ | $X_{2}$ | $d$ |\n",
    "| ------- | ------- | --- |\n",
    "| 0       | 0       | 0   |\n",
    "| 0       | 1       | 0   |\n",
    "| 1       | 0       | 0   |\n",
    "| 1       | 1       | 1   |\n",
    "\n",
    "**Puerta OR:**\n",
    "| $X_{1}$ | $X_{2}$ | $d$ |\n",
    "| ------- | ------- | --- |\n",
    "| 0       | 0       | 0   |\n",
    "| 0       | 1       | 1   |\n",
    "| 1       | 0       | 1   |\n",
    "| 1       | 1       | 1   |\n",
    "\n",
    "**Puerta XOR:**\n",
    "| $X_{1}$ | $X_{2}$ | $d$ |\n",
    "| ------- | ------- | --- |\n",
    "| 0       | 0       | 0   |\n",
    "| 0       | 1       | 1   |\n",
    "| 1       | 0       | 1   |\n",
    "| 1       | 1       | 0   |\n",
    "\n",
    "**Puerta NOT:**\n",
    "| $X_{1}$ | $d$ |\n",
    "| ------- | --- |\n",
    "| 0       | 1   |\n",
    "| 1       | 0   |\n",
    "\n",
    "Representa el error cometido en los cuatro patrones frente al número de épocas. La medida a representar es la suma de los 4 errores (al cuadrado) que se producen al pasar todos los patrones frente al número de repeticiones (épocas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos visto en teoría, el perceptró se basa en comparar la salida del sistema con una señal deseada. El algoritmo del perceptrón consiste en las siguientes etapas:\n",
    "\n",
    "1. Inicialización aleatoria de los coeficientes (pesos sinápticos) $w_{k}$.\n",
    "2. Inicialización del parámetro $\\alpha$.\n",
    "3. Obtención de la salida a partir del vector de entrada: $\\hat{y} = \\mathrm{signo}\\left( \\sum_{k=0}^{n} w_{k}x_{k} \\right)$\n",
    "4. Obtención del error: $\\varepsilon (\\hat{y}) = y - \\hat{y}$\n",
    "5. Actualización de los coeficientes: $w_{k} = w_{k} + \\alpha \\varepsilon(\\hat{y}) x_{k}$\n",
    "6. Vuelta al paso 3.\n",
    "\n",
    "Empezaremos definiendo la clase que contendrá los pesos de nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Opciones generales de Matplotlib para ajustar las figuras\n",
    "plt.rcParams['figure.figsize'] = [9, 4]\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "plt.rcParams['image.cmap'] = 'binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronSimple():\n",
    "    def __init__(self, input_shape, alpha):\n",
    "        ## Inicializamos la bias a 1 y los pesos aleatoriamente\n",
    "        self.w_0 = 1\n",
    "        self.w_k = np.random.normal(size=input_shape)\n",
    "        ## Guardamos el valor de alpha para la actualización de los pesos\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        El método __call__() es el que se ejecutará cuando hagamos PerceptronSimple(x).\n",
    "        Tiene que implementar el sumatorio del producto de los pesos por las entradas más la bias\n",
    "        activado con la función signo.\n",
    "        \"\"\"\n",
    "        return np.sign(np.sum(self.w_k*x, axis=-1) + self.w_0)\n",
    "\n",
    "    def update(self, error, x):\n",
    "        ## Actualizamos el valor de los pesos según la ecuación (5)\n",
    "        self.w_0 = self.w_0 + self.alpha*error\n",
    "        self.w_k = self.w_k + self.alpha*error*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos los vectores que contendrán las entradas al modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AND_OR_XOR = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "X_NOT = np.array([[0], [1]])\n",
    "\n",
    "## Recordemos que hay que codificar los 0s como -1s por la función signo\n",
    "Y_AND = np.array([-1, -1, -1, 1])\n",
    "Y_OR = np.array([-1, 1, 1, 1])\n",
    "Y_XOR = np.array([-1, 1, 1, -1])\n",
    "Y_NOT = np.array([1, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bucle de entrenamiento\n",
    "\n",
    "Como tenemos que entrenar un modelo distinto para cada puerta lógica, lo más cómodo es que creemos una función de entrenamiento que podamos reutilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, Y, epochs=20, verbose=True):\n",
    "    ## Creamos una lista para almacenar los errores de cada época.\n",
    "    ## De esta manera luego podemos representarlos.\n",
    "    train_error = []\n",
    "    for epoch in range(epochs):\n",
    "        ## También necesitamos una lista para almacenar los errores de cada muestra.\n",
    "        epoch_error = []\n",
    "        ## Iteramos sobre los ejemplos que tenemos\n",
    "        for X_i, Y_i in zip(X, Y):\n",
    "            ## Obtención de la predicción del modelo\n",
    "            Y_i_pred = model(X_i)\n",
    "            ## Cálculo del error\n",
    "            error = Y_i-Y_i_pred\n",
    "            epoch_error.append(error**2)\n",
    "            ## Actualización de los coeficientes\n",
    "            model.update(error, X_i)  \n",
    "        train_error.append(np.sum(epoch_error))\n",
    "        if verbose:\n",
    "            print(f\"Época {epoch+1} --> Error: {train_error[-1]}\")\n",
    "    return train_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos utilizar un bucle `for` para entrenar un modelo por cada puerta y comprobar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"AND\":{\"X\":X_AND_OR_XOR, \"Y\":Y_AND},\n",
    "    \"OR\":{\"X\":X_AND_OR_XOR, \"Y\":Y_OR},\n",
    "    \"XOR\":{\"X\":X_AND_OR_XOR, \"Y\":Y_XOR},\n",
    "    \"NOT\":{\"X\":X_NOT, \"Y\":Y_NOT},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for puerta_logica_nombre, puerta_logica_data in data.items():\n",
    "    model = PerceptronSimple(input_shape=puerta_logica_data[\"X\"].shape[-1], alpha=0.1)\n",
    "    results[puerta_logica_nombre] = {\n",
    "            'errores':train(model, puerta_logica_data[\"X\"], puerta_logica_data[\"Y\"], verbose=False),\n",
    "            'modelo':model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plots = len(results)\n",
    "plt.figure(figsize=(5*n_plots, 4))\n",
    "for i, (n, d) in enumerate(results.items(),1):\n",
    "    plt.subplot(1,n_plots,i)\n",
    "    plt.title(n)\n",
    "    plt.plot(d['errores'], 'k-')\n",
    "    plt.xlabel(\"Épocas\")\n",
    "    plt.ylabel(\"Error\")\n",
    "# plt.savefig(\"Images/PS_Losses.png\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el perceptrón simple es capaz de apreder todas las puertas lógicas menos la XOR. \n",
    "> ¿Por qué?\n",
    "XOR es una puerta lógica no linealmente separable, por lo que el perceptrón simple no es capaz de resolverlo. El resto de puertas sí que son todas linealmente separables y se resuelven correctamente con este modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dibujar la frontera de decisión\n",
    "\n",
    "Podemos dibujar la frontera de decisión para ver cómo nuestro modelo separa los diferentes puntos en el espacio.\n",
    "\n",
    "> La frontera de decisión se calcula igualando a 0 el producto de los pesos por las entradas más la bias: $w_{0} + \\sum_{k=1}^{N} w_{k}x_{k} = 0$. En nuestro caso bidimensional esto da lugar a la ecuación de una recta: $x_{1} = -\\frac{w_{2}}{w_{1}}x_{2} - \\frac{w_{0}}{w_{1}}$.\n",
    "\n",
    "Podemos crear una función genérica que nos calcule esta frontera de decisión para representarla cómodamente en todos los casos que vamos a estudiar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundary(model, xmin=-10, xmax=10):\n",
    "    if len(model.w_k)==2:\n",
    "        pendiente = -(model.w_k[1]/model.w_k[0])\n",
    "        offset =  - (model.w_0/model.w_k[0])\n",
    "    elif len(model.w_k)==1:\n",
    "        pendiente = 0\n",
    "        offset =  - (model.w_0/model.w_k[0])\n",
    "    x = np.linspace(xmin, xmax, 500)\n",
    "    return x, pendiente*x + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plots = len(results)\n",
    "plt.figure(figsize=(5*n_plots, 4))\n",
    "for i, (n, d) in enumerate(results.items(),1):\n",
    "    plt.subplot(1,n_plots,i)\n",
    "    plt.title(n)\n",
    "    if data[n]['X'].shape[-1] > 1:\n",
    "        sns.scatterplot(x=data[n]['X'][:,0], y=data[n]['X'][:,1], style=data[n]['Y'], hue=data[n]['Y'], palette=[\"blue\", \"red\"])\n",
    "    else:\n",
    "        sns.scatterplot(x=np.zeros_like(data[n]['X'][:,0]), y=data[n]['X'][:,0], style=data[n]['Y'], hue=data[n]['Y'], palette=[\"blue\", \"red\"])\n",
    "    plt.plot(*decision_boundary(d['modelo'], -1, 2), '--k', label=\"Frontera de decisión\")\n",
    "# plt.savefig(\"Images/Fronter_Perceptron.png\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobación de funciones linealmente separables.\n",
    "\n",
    "El perceptrón nos proporciona un modo indirecto para comprobar si una función es linealmente separable o no (comprobando la convergencia final del algoritmo). Usando el algoritmo implementado en el punto anterior determina si las siguientes funciones lógicas son linealmente separables o no ($\\bar{x}$ indica el conjugado de x; todas las variables son binarias y d es la señal deseada):\n",
    "\n",
    "* $d = \\mathrm{sign}(x_{1}x_{2}x_{3} + \\bar{x}_{1}x_{3} + \\bar{x}_{2}x_{1})$\n",
    "* $d = \\mathrm{sign}(x_{1}x_{2}x_{4} + \\bar{x}_{1}x_{4} + \\bar{x}_{3}x_{4} + x_{1}x_{2})$\n",
    "* $d = \\mathrm{sign}(x_{1}x_{2}\\bar{x}_{1}x_{4} + \\bar{x}_{2}x_{3} + \\bar{x}_{1}x_{4} + x_{2}x_{4})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADALINE. Algoritmo LMS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vuelve a determinar el primer apartado (salvo para la puerta XOR) pero usando el modelo ADALINE el algoritmo LMS (Least Mean Square); recuerda que en el perceptrón la salida se obtiene tras la función signo mientras que en el caso de la Adaline se considera la salida antes de la función signo. Si inicializas de forma diferente, ¿qué obtienes en el caso del perceptrón? ¿y en el caso de la Adaline?\n",
    "\n",
    "Este modelo queda representado por la siguiente figura: ![Adalina](Adalina.png) \n",
    "\n",
    "La principal diferencia con respecto al perceptrón es dónde se obtiene la realimentación del error; en este caso es antes del cálculo de la función signo. Este pequeño cambio, que parece sin importancia, va a permitir utilizar uno de los algoritmos que más uso tiene actualmente en los modelos de inteligencia artificial. El cambio en la realimentación permitió plantear un elemento que no se tenía antes con el perceptrón: una función de coste o de error. Esta función define la calidad del modelo. La característica que tiene es que un extremo suyo (máximo o mínimo) indica el funcionamiento óptimo del sistema. \n",
    "\n",
    "Se plantea un procedimiento iterativo para encontrar la solución conocido como descenso por gradiente. De forma intuitiva consiste en inicializar de forma aleatoria los parámetros y descender por la superficie de error hasta encontrar el mínimo de dicha función (si se busca el máximo se cambia el signo de la función y, entonces, el objetivo pasa a ser un mínimo). \n",
    "\n",
    "De forma análoga al algoritmo del perceptrón simple se tienen los siguientes pasos:\n",
    "1. Inicialización aleatoria de los coeficientes, pesos sinápticos, $w_{k}$. \n",
    "2. Inicialización del parámetro $\\alpha$. \n",
    "3. Obtención de la salida a partir del vector de entrada: $\\hat{y} = \\sum_{k=0}^{n}w_{k}x_{k}$\n",
    "4. Obtención del error: $\\varepsilon(y) = y - \\hat{y}$\n",
    "5. Actualización de los coeficientes $w_{k} = w_{k} + {α}\\varepsilon(y)x_{k}$\n",
    "6. Vuelta al paso 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adalina():\n",
    "    def __init__(self, input_shape, alpha):\n",
    "        ## Inicializamos la bias a 1 y los pesos aleatoriamente\n",
    "        self.w_0 = 1\n",
    "        self.w_k = np.random.normal(size=input_shape)\n",
    "        ## Guardamos el valor de alpha para la actualización de los pesos\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        El método __call__() es el que se ejecutará cuando hagamos Adalina(x).\n",
    "        Tiene que implementar el sumatorio del producto de los pesos por las \n",
    "        entradas más la bias activado con la función signo.\n",
    "        \"\"\"\n",
    "        return np.sum(self.w_k*x, axis=-1) + self.w_0\n",
    "\n",
    "    def update(self, error, x):\n",
    "        ## Actualizamos el valor de los pesos según la ecuación (5)\n",
    "        self.w_0 = self.w_0 + self.alpha*error\n",
    "        self.w_k = self.w_k + self.alpha*error*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"AND\":{\"X\":X_AND_OR_XOR, \"Y\":Y_AND},\n",
    "    \"OR\":{\"X\":X_AND_OR_XOR, \"Y\":Y_OR},\n",
    "    \"NOT\":{\"X\":X_NOT, \"Y\":Y_NOT},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adalina(model, X, Y, epochs=20, verbose=True):\n",
    "    ## Creamos una lista para almacenar los errores de cada época.\n",
    "    ## De esta manera luego podemos representarlos.\n",
    "    train_error = []\n",
    "    for epoch in range(epochs):\n",
    "        ## También necesitamos una lista para almacenar los errores de cada muestra.\n",
    "        epoch_error = []\n",
    "        ## Calculamos todas las predicciones de golpe y calculamos el error cudrático\n",
    "        ## medio de todo el batch.\n",
    "        for X_i, Y_i in zip(X, Y):\n",
    "            ## Obtención de la predicción del modelo\n",
    "            Y_i_pred = model(X_i)\n",
    "            ## Cálculo del error\n",
    "            error = Y_i-Y_i_pred\n",
    "            epoch_error.append(error**2)\n",
    "            ## Actualización de los coeficientes\n",
    "            model.update(error, X_i)  \n",
    "        train_error.append(np.sum(epoch_error))\n",
    "        if verbose:\n",
    "            print(f\"Época {epoch+1} --> Error: {train_error[-1]}\")\n",
    "    return train_error  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for puerta_logica_nombre, puerta_logica_data in data.items():\n",
    "    model = Adalina(input_shape=puerta_logica_data[\"X\"].shape[-1], alpha=0.1)\n",
    "    results[puerta_logica_nombre] = {\n",
    "            'errores':train_adalina(model, puerta_logica_data[\"X\"], puerta_logica_data[\"Y\"], epochs=50, verbose=False),\n",
    "            'modelo':model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos aperciar que en este caso, al no utilizar la función signo para determinar las salidas, las funciones de pérdida tienen una forma contínua que baja suavemente hasta un mínimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plots = len(results)\n",
    "plt.figure(figsize=(5*n_plots, 4))\n",
    "for i, (n, d) in enumerate(results.items(),1):\n",
    "    plt.subplot(1,n_plots,i)\n",
    "    plt.title(n)\n",
    "    plt.plot(d['errores'], 'k-')\n",
    "    plt.xlabel(\"Épocas\")\n",
    "    plt.ylabel(\"Error\")\n",
    "# plt.savefig(\"Images/losses_adalina.png\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto permite, entre otras cosas, que las fronteras de decisión sean más suaves y se adapten mejor a los distintos problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plots = len(results)\n",
    "plt.figure(figsize=(5*n_plots, 4))\n",
    "for i, (n, d) in enumerate(results.items(),1):\n",
    "    plt.subplot(1,n_plots,i)\n",
    "    plt.title(n)\n",
    "    if data[n]['X'].shape[-1] > 1:\n",
    "        sns.scatterplot(x=data[n]['X'][:,0], y=data[n]['X'][:,1], style=data[n]['Y'], hue=data[n]['Y'], palette=[\"blue\", \"red\"])\n",
    "    else:\n",
    "        sns.scatterplot(x=np.zeros_like(data[n]['X'][:,0]), y=data[n]['X'][:,0], style=data[n]['Y'], hue=data[n]['Y'], palette=[\"blue\", \"red\"])\n",
    "    plt.plot(*decision_boundary(d['modelo'], -1, 2), '--k', label=\"Frontera de decisión\")\n",
    "# plt.savefig(\"Images/frontera_adalina.png\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación práctica I. Algoritmo LSM. Eliminación variación línea base.\n",
    "\n",
    "Vamos a ver la aplicación de un filtro adaptativo de un solo coeficiente. Se plantea un esquema típico de cancelación de eco como se muestra en la figura:\n",
    "\n",
    "![Cancelacion_Eco](Cancelacion_Eco.png)\n",
    "\n",
    "Nos planteamos eliminar la variación que suponemos muy lenta, por lo que asumiremos que dicha variación es igual a un valor constante; ese valor lo representaremos por $w_{0}$, siendo la salida del filtro adaptativo igual a a $w_{0}\\times 1$. Vamos a implementar el algoritmo LMS en este caso conociendo que:\n",
    "\n",
    "a) $d(n)$ es la señal deseada, en este caso es la señal del electrocardiograma que se encuentra en el fichero `vlb.mat`.\n",
    "b) Lo que se busca representar es el error cometido, $e(n)$, en cada iteración.\n",
    "c) Comprueba el funcionamiento del sistema para diferentes constantes de adaptación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancelador Activo de Ruido\n",
    "\n",
    "La estructura de un cancelador activo de ruido viene dada por el siguiete esquema:\n",
    "\n",
    "![Cancelador_Activo_Ruido](Cancelador_Activo_Ruido.png)\n",
    "\n",
    "Un ejemplo clásico de esta estructura es la obtención del electrocardiograma fetal teniendo como principal interferencia el electrocardiograma materno. El problema es que estas dos señales tienen un contenido espectral similar por lo que intentar un filtrado selectivo en frecuencia supone distorsionar la señal de interés; en este caso el electrocardiograma fetal. Como solución se plantea un cancelador activo de ruido donde la señal deseada es la señal que se obtiene del abdomen de la madre (tiene componentes maternos y fetales) y la señal de entrada al filtro adaptativo es la señal que se obtiene del sensor que se le coloca en el pecho a la madre; este sensor tomaría, en principio, solo componente materna. El sistema adaptativo modelizaría la componente materna que se encuentra en el sensor que se coloca en el abdomen de la madre de tal forma que la señal de error resultante sólo tendría componente fetal. El fichero `fetal_lms.mat` contiene dos señales; la torácica y la abdominal; la torácica será la entrada al filtro adaptativo y la abdominal es la que contiene las dos componentes (fetal y materna modificada). Determina:\n",
    "\n",
    "a)  La visualización de ambas señales.\n",
    "\n",
    "b) Implementa el algoritmo LMS en esta aplicación (puedes usar el código del punto anterior cambiando las señales de entrada al filtro y la señal deseada); varía la longitud del filtro y la constante de adaptación.\n",
    "\n",
    "c) Visualiza el error cometido; ¿qué ocurre con el complejo QRS (¡los picos!) de la madre en la señal de salida?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrón multicapa\n",
    "\n",
    "En esta sección implementaremos un perceptrón multicapa y lo utilizaremos posteriormente para resolver una tarea de clasficación y otra de regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de las funciones de activación y sus derivadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_fn(x):\n",
    "    return x\n",
    "def dlinear_fn(x):\n",
    "    return np.ones_like(x)\n",
    "def relu_fn(x):\n",
    "    x[x<0] = 0\n",
    "    return x\n",
    "def drelu_fn(x):\n",
    "    x[x<0] = 0\n",
    "    x[x>=0] = 1\n",
    "    return x\n",
    "def sigmoid_fn(x):\n",
    "    # Tenemos que clipear los valores de entrada para evitar el overflow\n",
    "    x = np.clip(x, -500, 500)\n",
    "    return 1.0/(1+np.exp(-x))\n",
    "def dsigmoid_fn(x):\n",
    "    loss = sigmoid_fn(x)*(1-sigmoid_fn(x))\n",
    "    loss = np.clip(loss, -100, 100)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de las funciones de coste y sus derivadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(Y_true, Y_pred):\n",
    "    return (Y_true - Y_pred).sum(axis=-1)**2\n",
    "def dmse(Y_true, Y_pred):\n",
    "    return -2*(Y_true - Y_pred)\n",
    "\n",
    "## En el caso de la BCE añadimos un valor muy pequeño a los logaritmos y las divisiones\n",
    "## para asegurarnos de que no se produce ni log(0) ni divisiones por 0.\n",
    "def bce(Y_true, Y_pred):\n",
    "    return -(Y_true*np.log(Y_pred + 1e-5)+(1-Y_true)*np.log(1-Y_pred + 1e-5)).sum(axis=-1)\n",
    "def dbce(Y_true, Y_pred):\n",
    "    return ((Y_pred-Y_true)/(Y_pred*(1-Y_pred) + 1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Definición de las clases\n",
    "\n",
    "Para hacer el código lo más limpio y reutilizable posible vamos a crear dos clases:\n",
    "\n",
    "- `Layer`: Representará una única capa del perceptrón y almacenará toda la información necesaria para su uso y el entrenamiento, como los pesos, las funciones de activación, las salidas y los valores de $\\delta$ que se usarán para actualizar los pesos.\n",
    "- `LayerStack`: Representará una apilación de capas `Layer` y permitirá realizar el entrenamiento y la inferencia de forma cómoda mediante los métodos `backpropagate()`, `update()` y `fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, input_shape, neurons, activation='linear'):\n",
    "        self.w_0 = np.ones(shape=(neurons,1))\n",
    "        self.w_k = np.random.normal(size=(input_shape, neurons)).T\n",
    "        self.activation, self.dactivation = self.set_activation(activation)\n",
    "        self.z_k = None\n",
    "        self.a_k = None\n",
    "        self.delta_k = None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Trasponemos para que sea consistente con la teoría, ya que espera un vector columna\n",
    "        z_k = self.w_k @ x.T  + self.w_0\n",
    "        a_k = self.activation(z_k)\n",
    "\n",
    "        self.z_k = z_k\n",
    "        self.a_k = a_k\n",
    "\n",
    "        return a_k.T\n",
    "\n",
    "    def set_activation(self, activation):\n",
    "        possible_activations = {\n",
    "            'linear': linear_fn,\n",
    "            'relu': relu_fn,\n",
    "            'sigmoid': sigmoid_fn\n",
    "        }\n",
    "        possible_dactivations = {\n",
    "            'linear': dlinear_fn,\n",
    "            'relu': drelu_fn,\n",
    "            'sigmoid': dsigmoid_fn\n",
    "        }\n",
    "\n",
    "        return possible_activations[activation], possible_dactivations[activation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la última capa calculamos\n",
    "\n",
    "$$\n",
    "\\delta_{k}^{L=F} = 2(y_{k} - \\hat{y}_{k})^{2}f'(z_{k}^{L=F})\n",
    "$$\n",
    "\n",
    "Esto nos lleva a la ecuación de actualización:\n",
    "\n",
    "$$\n",
    "w_{kj}^{L=F} = w_{kj}^{L=F} - \\alpha \\delta_{k}^{L=F}a_{j}^{L=F-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la capas ocultas lo podemos calcular iterativamente partiendo de la última capa y propagando hacia detrás:\n",
    "\n",
    "$$\n",
    "\\delta_{k}^{L} = f'(z_{k}^{L})\\sum_{j}\\delta_{j}^{L+1}w_{jk}^{L}\n",
    "$$\n",
    "\n",
    "Dónde los pesos se actualizan siguiendo la misma regla que antes (teniendo en cuenta que ahora la delta la calcularemos de forma distinta):\n",
    "\n",
    "$$\n",
    "w_{kj}^{L} = w_{kj}^{L} - \\alpha \\delta_{k}^{L}a_{j}^{L-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerStack():\n",
    "    def __init__(self, input_shape, neurons, activations, alpha, loss):\n",
    "        self.layers = self.generate_layers(input_shape, neurons, activations)\n",
    "        self.alpha = alpha\n",
    "        self.loss_fn = mse if loss=='mse' else (bce if loss=='bce' else print(\"El parámetro loss tiene que ser 'mse' o 'bce'.\"))\n",
    "        self.dloss_fn = dmse if loss=='mse' else (dbce if loss=='bce' else print(\"El parámetro loss tiene que ser 'mse' o 'bce'.\"))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def generate_layers(self, input_shape, neurons, activations):\n",
    "        layers = []\n",
    "        ## Creamos la primera capa y la metemos en la lista de capas\n",
    "        input_layer = Layer(input_shape=input_shape, neurons=neurons[0], activation=activations[0])\n",
    "        layers.append(input_layer)\n",
    "\n",
    "        ## Creamos ahora las capas ocultas\n",
    "        for i, (input_dim, output_dim) in enumerate(zip(neurons[0:-1], neurons[1:-1]), 1):\n",
    "            layer = Layer(input_shape=input_dim, neurons=output_dim, activation=activations[i])\n",
    "            layers.append(layer)\n",
    "            \n",
    "        ## Finalmente añadimos la última capa\n",
    "        output_layer = Layer(input_shape=neurons[-2], neurons= neurons[-1], activation=activations[-1])\n",
    "        layers.append(output_layer)\n",
    "        return layers\n",
    "\n",
    "    def backpropagate(self, Y_true, Y_pred):\n",
    "        ## Calculamos la delta final \n",
    "        delta_F_k = self.dloss_fn(Y_true, Y_pred)*self.layers[-1].dactivation(self.layers[-1].z_k)\n",
    "        self.layers[-1].delta_k = delta_F_k\n",
    "\n",
    "        ## Utilizamos la delta final para calcular la delta de cada capa\n",
    "        for i, layer in reversed(list(enumerate(self.layers[1:]))):\n",
    "            delta_k = self.layers[i].dactivation(self.layers[i].z_k)*layer.w_k.T @ layer.delta_k\n",
    "            self.layers[i].delta_k = delta_k.copy()\n",
    "\n",
    "    def update(self, X):\n",
    "        ## Actualizamos los pesos de la primera capa teniendo en cuenta que su input es X_i\n",
    "        self.layers[0].w_k = self.layers[0].w_k - self.alpha*self.layers[0].delta_k @ X\n",
    "        self.layers[0].w_0 = self.layers[0].w_0 - self.alpha*self.layers[0].delta_k\n",
    "\n",
    "        ## Actualizamos los pesos menos los de la primera capa\n",
    "        for L in range(1, len(self.layers)):\n",
    "            self.layers[L].w_k = self.layers[L].w_k - self.alpha*self.layers[L].delta_k @ self.layers[L-1].a_k.T\n",
    "            self.layers[L].w_0 = self.layers[L].w_0 - self.alpha*self.layers[L].delta_k \n",
    "\n",
    "    def fit(self, X, Y, epochs, verbose=True):\n",
    "        losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            losses_epoch = []\n",
    "\n",
    "            for X_i, Y_i in zip(X, Y):\n",
    "                ## Añadimos una dimensión por consistencia con la teoría\n",
    "                X_i = np.expand_dims(X_i,0)\n",
    "\n",
    "                ## Calculamos una predicción y su error\n",
    "                pred = self(X_i)\n",
    "                loss = self.loss_fn(Y_true=Y_i, Y_pred=pred)\n",
    "                losses_epoch.append(loss)\n",
    "                \n",
    "                ## Backpropagation para calcular las deltas\n",
    "                self.backpropagate(Y_true=Y_i, Y_pred=pred)\n",
    "\n",
    "                ## Actualizamos los pesos\n",
    "                self.update(X_i)\n",
    "                \n",
    "            losses.append(np.mean(losses_epoch))\n",
    "            if verbose:\n",
    "                print(f\"Época {epoch+1}: [Loss] {losses[-1]}\")\n",
    "                \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP usado como clasificador\n",
    "\n",
    "En este apartado vamos a usar el perceptrón multicapa en un problema de clasificación clínica. Para ello usaremos un conjunto de datos de salud que se puede considerar como un clásico, el de cáncer de mama de Wisconsin y que está accesible en [dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29) (además del repositorio del presente libro). Es un conjunto de datos con 32 entradas y una salida que define el tipo de tumor.\n",
    "\n",
    "> Lo más sencillo es obtener el dataset directamente de la librearía `sklearn`.\n",
    "\n",
    "Para resolver un problema de clasificación con un MLP tenemos que tener en consideración que la salida de la última capa tiene que representar una probabilidad y que tenemos que utilizar una función de coste acorde. Esto lo conseguimos utilizando, en la última capa, la función de activación sigmoide, que obliga a que nuestra salida esté contenida en el intervalo $[0,1]$ de forma que se puede interpretar como una probabilidad. Acorde con esto se utiliza la entropía cruzada binaria (Binary CrossEntropy) como función de coste. Esta función no se encarga solamente de penalizar las predicciones errones, también penaliza una predicción correcta pero poco segura. Al optimizarla estamos buscando un modelo que acierte y que, además, lo haga con seguridad. Tiene esta forma:\n",
    "\n",
    "$$\n",
    "J(y,\\hat{y}) = \\sum_{i}^{N} -y\\mathrm{log}(\\hat{y}) - (1-y)\\mathrm{log}(1-\\hat{y})\n",
    "$$\n",
    "\n",
    "Y su derivada:\n",
    "\n",
    "$$\n",
    "\\frac{\\delta J}{\\delta \\hat{y}} = \\frac{\\hat{y}-y}{\\hat{y}(1-\\hat{y})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de los datos\n",
    "\n",
    "Los datos que vamos a utilizar los podemos descargar también directamente desde la librería `sklearn` mediante el parámetro `load_breast_cancer()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_breast_cancer(return_X_y=True)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es muy importante comprobar las distribuciones de las variables que vamos a introducir al modelo. Idealmente deberían estar todas en el rango $[-1,1]$ para asegurar la convergencia del modelo. En la siguiente figura se observa que esto no se cumple para todas las variables, así que lo mejor que podemos hacer es estandarizarlas restándoles su media y dividiendo por su desviación estándar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "for i in range(X.shape[-1]):\n",
    "    plt.subplot(5,6,i+1)\n",
    "    plt.hist(X[:,i], color='k')\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "plt.suptitle(\"Distribuciones de las variables\")\n",
    "# plt.savefig(\"Images/distribuciones_cancer_negro.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo más correcto es dividir primero los datos en un conjunto de entrenamiento y un conjunto de test y utilizar las medias y distribuciones estándar del conjunto de entrenamiento para estandarizara los dos conjuntos. Así nos aseguramos de que no se filtra ningún tipo de información del conjunto de test al conjunto de entrenamiento y podemos estar tranquilos de que las métricas que obtengamos son fiables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('X_phrase.csv')\n",
    "X = dataframe.iloc[:, 3:-2].values\n",
    "Y = dataframe.iloc [:, 6373]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1385, 6370), (1385,), (594, 6370), (594,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, stratify=Y, random_state=42)\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = X_train.mean(axis=0)\n",
    "train_std = X_train.std(axis=0)\n",
    "\n",
    "X_train_std = (X_train - train_mean)/train_std\n",
    "X_test_std = (X_test - train_mean)/train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1: [Loss] 3.020076065822101\n",
      "Época 2: [Loss] 1.523595184339309\n",
      "Época 3: [Loss] 1.2158931376675837\n",
      "Época 4: [Loss] 1.140858002938852\n",
      "Época 5: [Loss] 1.0267999967404307\n",
      "Época 6: [Loss] 0.9358672418560368\n",
      "Época 7: [Loss] 0.879426275042678\n",
      "Época 8: [Loss] 0.780095845655356\n",
      "Época 9: [Loss] 0.7956657007311114\n",
      "Época 10: [Loss] 0.6684291096323893\n",
      "Época 11: [Loss] 0.5658700776300379\n",
      "Época 12: [Loss] 0.7400885530814592\n",
      "Época 13: [Loss] 0.6861448732563239\n",
      "Época 14: [Loss] 0.6408813794878501\n",
      "Época 15: [Loss] 0.5272414144669367\n",
      "Época 16: [Loss] 0.5921066593166521\n",
      "Época 17: [Loss] 0.5756531041180999\n",
      "Época 18: [Loss] 0.5638065169727677\n",
      "Época 19: [Loss] 0.6068227982858598\n",
      "Época 20: [Loss] 0.5901838036566993\n"
     ]
    }
   ],
   "source": [
    "model = LayerStack(X_train_std.shape[-1], [30 ,1], ['relu', 'sigmoid'], alpha=0.01, loss='bce')\n",
    "loss = model.fit(X_train_std, Y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss, 'k-o')\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Loss\")\n",
    "# plt.savefig(\"loss_cancer.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La evolución de la función de coste indica que el modelo es capaz de ir aprendiendo a predecir la variable objetivo. Tambibén podemos calcular, por ejemplo, la precisió para el conjunto de entrenamiento y el conjunto de test, donde observamos un resultado bastante bueno aunque sufriendo un poco de sobreentrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión -> [Entrenamiento] 0.95% [Test] 0.91%\n"
     ]
    }
   ],
   "source": [
    "pred_train = model(X_train_std)\n",
    "pred_train = np.where(pred_train > 0.5, 1, 0)\n",
    "pred_test = model(X_test_std)\n",
    "pred_test = np.where(pred_test > 0.5, 1, 0)\n",
    "\n",
    "print(f\"Precisión -> [Entrenamiento] {accuracy_score(Y_train, pred_train):.2f}% [Test] {accuracy_score(Y_test, pred_test):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP usado como modelizador\n",
    "\n",
    "En este apartado se plantea usar un perceptrón multicapa para predecir la progresión de la diabetes en un año para diferentes pacientes. Las dos diferencias principales respecto a un problema de clasificación como el anterior son:\n",
    "\n",
    "1. La activación de la última capa.\n",
    "2. La función de coste.\n",
    "\n",
    "En un problema de clasificación lo normal es utilizar una función sigmoide como activación de la última capa para que el valor proporcionado por el modelo esté entre 0 y 1, y se pueda interpretar como una probabilidad. En cambio, lo que queremos predecir ahora será simplemente un valor numérico que puede tomar cualquier valor, asi que utilizaremos una función de activación lineal $f(x)=x$.\n",
    "Respecto a la función de coste, en los problemas de clasificación se utiliza la entropía cruzada, mientras que en problemas de regresión lo que queremos es asegurarnos de que nuestras predicciones están lo más cerca posible del valor real, así que utilizamos el error cuadrático medio (*Mean Squared Error*). También es posible utilizar el arror absoluto medio (*Mean Absolute Error*) en lugar del cuadrático, pero este último fuerza al modelo a corregir más las predicciones muy alejadas de la real, por lo que suele dar mejores resultados.\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE} = \\sum_{i}^{N} (y_{i} - \\hat{y_{i}})^{2}\n",
    "$$\n",
    "\n",
    "Este cambio en la función de coste solamente afecta al cálculo de la $\\delta^{L=F}$, ya que ahí interviene la derivada de la función de coste. La derivada del MSE es simplemente $\\frac{\\partial J}{\\partial w_k} = -2(y-\\hat{y})\\frac{\\partial \\hat{y}}{\\partial w_k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de los datos\n",
    "\n",
    "Los datos que vamos a utilizar los podemos descargar también directamente desde la librería `sklearn` mediante el parámetro `load_diabetes()`.\n",
    "\n",
    "> Estos datos ya han sido centrados y escalados por la desviación estándar por el número de muestras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_diabetes(return_X_y=True)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos representar las distribuciones de las variables para asegurarnos de que todas están contenidas en el rango deseado. Recordemos que para trabajar con un perceptrón multicapa es recomendable que las características de entrada estén contenidas entre -1 y 1 para facilitar la convergencia del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = [\"age\", \"sex\", \"bmi\", \"bp\", \"s1\", \"s2\", \"s3\", \"s4\", \"s5\", \"s6\"]\n",
    "plt.figure()\n",
    "for i in range(X.shape[-1]):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.title(colnames[i])\n",
    "    plt.hist(X[:,i], color='k')\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "plt.suptitle(\"Distribuciones de las variables\")\n",
    "# plt.savefig(\"Images/distribuciones_diabetes_black.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viendo esto podríamos pensar que nuestros datos están listos, ¡pero nos falta por comprobar la distribución de la variable objetivo! Esto es extremandamente importante, ya que si no transformamos también la variable objetivo para que esté en el mismo rango nuestro modelo será incapaz de aprender a predecirla. Pensemos que hay que realizar un montón de iteraciones para actualizar los pesos de forma que al multiplicarlos por un numero muy pequeño se obtenga un número muy grande. Esta transformación la realizaremos una vez tengamos separados los datos en conjunto de entrenamiento y test, ya que tendremos que transformar los valores del conjunto de test con los parámetros que obtengamos en el conjunto de entrenamiento. De esta forma se evita el goteo de información entre entrenamiento y test, que puede hacernos obtener unas métricas demasiado optimistas que no representen el rendimiento real de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Variable objetivo\")\n",
    "plt.hist(Y, color='k')\n",
    "# plt.savefig(\"Images/variable_objetivo_diabetes.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separación en conjunto de entrenamiento y conjunto de test\n",
    "\n",
    "Una vez hemos cargado los datos y hemos comprobado que todos están en el rango deseado tenemos que separarlos en los conjuntos de entrenamiento y test. De esta forma podremos tener un conjunto de datos separado con el que comprobar la capacidad de generalización del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí, transformamos la variable objetivo restando la media y diviendo por la desviación estándar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_Y = Y_train.mean(axis=0)\n",
    "std_Y = Y_train.std(axis=0)\n",
    "Y_train_std = (Y_train - mean_Y)/std_Y\n",
    "Y_test_std = (Y_test - mean_Y)/std_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comprobar que ahora los rangos de la variable objetivo, tanto en el conjunto de entrenamiento como en el de test, están mucho más cerca del rango deseado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.suptitle(\"Variable objetivo\")\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Conjunto de entrenamiento\")\n",
    "plt.hist(Y_train_std, color='k')\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Conjunto de test\")\n",
    "plt.hist(Y_test_std, color='k')\n",
    "# plt.savefig(\"Images/train_test_diabetes_black.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo\n",
    "\n",
    "Finalmente solamente nos queda entrenar el modelo y comprobar cómo de bien somos capaces de modelizar los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LayerStack(X_train.shape[-1], [15, 15, 1], ['relu', 'relu', 'linear'], alpha=0.001, loss='mse')\n",
    "loss = model.fit(X_train, Y_train_std, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss, 'k-o')\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig(\"loss_diabetes.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La evolución de la función de coste indica que el modelo es capaz de ir aprendiendo a predecir la variable objetivo. Tambibén podemos calcular, por ejemplo, el MAE para el conjunto de entrenamiento y el conjunto de test. Hay que tener en cuenta que hemos transformado la variable objetivo, por lo que si queremos obtener el MAE respecto a la escala original tenemos que deshacer esta transformación:\n",
    "\n",
    "$$\n",
    "Y_{o} = Y_{std}\\sigma_{Y} + \\mu_{Y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model(X_train)\n",
    "pred_test = model(X_test)\n",
    "\n",
    "pred_train_o = pred_train*std_Y + mean_Y\n",
    "pred_test_o = pred_test*std_Y + mean_Y\n",
    "\n",
    "print(f\"MAE -> [Entrenamiento] {mean_absolute_error(Y_train_std, pred_train):.2f} [Test] {mean_absolute_error(Y_test_std, pred_test):.2f}\")\n",
    "print(f\"MAE -> [Entrenamiento] {mean_absolute_error(Y_train, pred_train_o):.2f} [Test] {mean_absolute_error(Y_test, pred_test_o):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ee7271b61b99a85d1871c99316625859e21bab1bd159828f7866616625084c6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
